{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm5KiW9w9JNTitB0yMEifk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbk9889/Speech-to-text/blob/main/fine_tuning_Whisper_turbo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k95uEx5j2jb"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install dependencies\n",
        "!pip install -q transformers datasets accelerate peft librosa soundfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import libraries\n",
        "import os\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from transformers import (\n",
        "    WhisperProcessor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")"
      ],
      "metadata": {
        "id": "H36eemjTmMw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define model and processor\n",
        "model_id = \"openai/whisper-large-v3-turbo\"  # Using the turbo version\n",
        "processor = WhisperProcessor.from_pretrained(model_id)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,      # use FP16 if GPU available\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "X3TAWDSJmT42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Prepare your custom dataset\n",
        "# Assume you have a CSV file (e.g., \"train.csv\" and \"test.csv\") with columns:\n",
        "# \"audio_filepath\" (full path to the audio file) and \"transcript\" (the ground truth text)\n",
        "data_files = {\"train\": \"data/train.csv\", \"test\": \"data/test.csv\"}\n",
        "dataset = load_dataset(\"csv\", data_files=data_files)\n"
      ],
      "metadata": {
        "id": "Pb7nBqY-mbB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Load and resample audio files\n",
        "def load_audio(example):\n",
        "    # Load the audio file with librosa and force sampling rate to 16kHz (the expected rate)\n",
        "    audio, sr = librosa.load(example[\"audio_filepath\"], sr=16000)\n",
        "    example[\"audio_array\"] = audio\n",
        "    example[\"sampling_rate\"] = sr\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(load_audio)"
      ],
      "metadata": {
        "id": "oKuwUboamd8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Prepare dataset samples for training\n",
        "def prepare_sample(batch):\n",
        "    # Process audio to input_features (log-Mel spectrogram)\n",
        "    inputs = processor(batch[\"audio_array\"], sampling_rate=batch[\"sampling_rate\"])\n",
        "    batch[\"input_features\"] = inputs.input_features[0]\n",
        "    # Tokenize the transcription; note that the tokenizer pads/truncates automatically later\n",
        "    batch[\"labels\"] = processor.tokenizer(batch[\"transcript\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "EiSf397omgtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove unnecessary columns after mapping\n",
        "remove_columns = [\"audio_filepath\", \"transcript\", \"audio_array\", \"sampling_rate\"]\n",
        "dataset = dataset.map(prepare_sample, remove_columns=remove_columns)\n"
      ],
      "metadata": {
        "id": "7_Qee_4WmiuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define TrainingArguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-finetuned\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    num_train_epochs=3,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    logging_steps=100,\n",
        "    learning_rate=1e-5,\n",
        "    fp16=True,\n",
        ")"
      ],
      "metadata": {
        "id": "lhp020EGmk6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Create a data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    tokenizer=processor.tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "QyZnqzwwmnAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Initialize Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=processor.feature_extractor,  # used for padding input_features\n",
        ")"
      ],
      "metadata": {
        "id": "_0DlBHFemoku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Fine-tune the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "K0wgWPNomriG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Save the fine-tuned model and processor\n",
        "trainer.save_model(\"./whisper-finetuned\")\n",
        "processor.save_pretrained(\"./whisper-finetuned\")"
      ],
      "metadata": {
        "id": "DUhwD2bbmtO-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}